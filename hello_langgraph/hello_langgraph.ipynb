{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857f4a67",
   "metadata": {},
   "source": [
    "### LM Studio Model List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1260e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (2.15.0)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: colorama in c:\\project\\aiot\\github\\langgraph_rag\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "\n",
      "   ---------------------------------------- 0/2 [python-dotenv]\n",
      "   -------------------- ------------------- 1/2 [dotenv]\n",
      "   ---------------------------------------- 2/2 [dotenv]\n",
      "\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fab832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "print(\"Models:\", [m.id for m in models.data])\n",
    "\n",
    "def chat_local(prompt: str, model: str = None, temperature: float = 0.2) -> str:\n",
    "    \"\"\" LM Studio compatiable Chat Completions function call\"\"\"\n",
    "    model = model or (models.data[0].id if models.data else \"meta-llam-3.1-8b-instruct\")\n",
    "    resp = client.chat.completions.create(\n",
    "        mode=model,\n",
    "        message=[{\"role\":\"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77af29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"no OPENAI_API_KEY\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "DEFAULT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def chat_local(prompt: str, model: str = None, temperature: float = 0.2) -> str:\n",
    "    \"\"\"Call OpenAI API\"\"\"\n",
    "    model = model or DEFAULT_MODEL\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4e47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâ€™m an AI language model created by OpenAI, designed to assist with a wide range of questions and tasks. I can provide information, answer queries, and help with writing, learning, and more. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(chat_local(\"Introduce yourslef shortly.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e14d9",
   "metadata": {},
   "source": [
    "### Hello LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip  install langgraph TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316103d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a106cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "class S1(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "g1 = StateGraph(S1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7751005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node -> Agent\n",
    "def echo_node(state: S1) -> S1:\n",
    "    return { # return -> state update\n",
    "        \"output\": f\"Echo: {state[\"input\"]}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node add\n",
    "g1.add_node(\"echo\", echo_node)\n",
    "\n",
    "# Edge add\n",
    "g1.add_edge(START, \"echo\")\n",
    "g1.add_edge(\"echo\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app1 = g1.compile()\n",
    "\n",
    "app1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app1.invoke({\n",
    "    \"input\": \"LangGraph first graph\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a77489",
   "metadata": {},
   "source": [
    "### LLM Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ac2b6",
   "metadata": {},
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "class S2(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "g2 = StateGraph(S2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node define\n",
    "def echo_node(state: S2) -> S2:\n",
    "    print(\"Echo node ->\", state)\n",
    "    return { # return -> state update\n",
    "        \"question\": f\"Echo: {state[\"answer\"]}\",\n",
    "    }\n",
    "\n",
    "def llm_node(state: S2) -> S2:\n",
    "    ans = chat_local(f\"introduce youself: {state[\"question\"]}\")\n",
    "    print(\"LLM node ->\", state)\n",
    "    return {\n",
    "        \"anser\": ans\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.add_node(\"echo\", echo_node)\n",
    "g2.add_node(\"llm\", llm_node)\n",
    "\n",
    "g2.add_edge(START, \"echo\")\n",
    "g2.add_edge(\"echo\", \"llm\")\n",
    "g2.add_edge(\"llm\", END)\n",
    "\n",
    "app2 = g2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "app2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a907d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "app2.invoke({\n",
    "    \"question\": \"What is LangGraph\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336340bb",
   "metadata": {},
   "source": [
    "### Conditional Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "def validatation(state: S3) -> S3:\n",
    "    q = (state.get(\"question\") or \"\").strip()\n",
    "    return {\n",
    "        \"result\": \"Empty question\"\n",
    "    }\n",
    "\n",
    "def answer(state: S3) -> S3:\n",
    "    q = state[\"answer\"]\n",
    "    a = chat_local(f\"short answer: {q}\")\n",
    "    return {\n",
    "        \"result\": a\n",
    "    }\n",
    "\n",
    "def route_fn(state: S3) -> str:\n",
    "    if not (state.get(\"question\") or \"\").strip():\n",
    "        return \"empty\"\n",
    "    return \"ok\"\n",
    "\n",
    "g3 = StateGraph(S3)\n",
    "g3.add_node(\"validate\", validatation)\n",
    "g3.add_node(\"answer\", answer)\n",
    "\n",
    "g3.add_edge(START, \"validate\")\n",
    "g3.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    route_fn,\n",
    "    {\n",
    "        \"empty\": END,\n",
    "        \"ok\": \"answer\"\n",
    "    }\n",
    ")\n",
    "g3.add_edge(\"answer\", END)\n",
    "\n",
    "app3 = g3.compile()\n",
    "app3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c288d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app3.invoke({\"question\":\"\"}))\n",
    "print(app3.invoke({\"question\":\"Contional Edge?\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
